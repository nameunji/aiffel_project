{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc247ff2",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df21c3",
   "metadata": {},
   "source": [
    "# Transformer 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87965120",
   "metadata": {},
   "source": [
    "```\n",
    "$ pip install transformers\n",
    "\n",
    "$ git clone https://github.com/huggingface/transformers.git\n",
    "$ cd transformers\n",
    "$ pip install -e .\n",
    "\n",
    "// 재부팅 명령어\n",
    "$ sudo reboot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606afd8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:36.991758Z",
     "start_time": "2021-05-08T04:13:25.250931Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978193640708923}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# transformer 잘 설치되었는지 확인\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0ada8",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea1bcd",
   "metadata": {},
   "source": [
    "# Huggingface transformers 설계구조 개요\n",
    "1. task 정의 후 그에 맞게 dataset 가공\n",
    "2. 적당한 model을 선택하고 설계\n",
    "3. model 학습\n",
    "4. model 학습을 통해 나온 weight와 config들을 저장\n",
    "5. 저장한 model의 checkpoint는 배포하거나 evaluation 할 때 사용\n",
    "\n",
    "<br>\n",
    "\n",
    "**transformers 설계**\n",
    "\n",
    "|||\n",
    "|:-|:-|\n",
    "|Processors | task 정의, dataset 가공|\n",
    "|Tokenizer | 텍스트 데이터 전처리|\n",
    "|Model | model 정의|\n",
    "|Optimization | optimizer와 학습 schedule(warm up 등)을 관리|\n",
    "|Trainer | 학습 과정을 전반적으로 관리|\n",
    "|Config | weight와 tokenizer, model을 쉽게 불러올 수 있도록 각종 설정 저장|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa0927",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# 1. Model\n",
    "model은 2가지 방식으로 불러올 수 있다.\n",
    "\n",
    "## 1-1. task에 적합한 모델을 직접 선택하여 import  \n",
    "\n",
    "모델을 로드할 때는 `from_pretrained`라는 메소드를 사용하며,  Huggingface의 pretrained 모델을 불러올 수도, 직접 학습시킨 모델을 불러올 수도 있다.\n",
    "- Huggingface에서 제공하는 pretrained 모델 : 모델의 이름을 string으로 전달\n",
    "- 직접 학습시킨 모델 : config와 모델을 저장한 경로를 string으로 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9abc429d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:40.016644Z",
     "start_time": "2021-05-08T04:13:36.993076Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForPreTraining\n",
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250b9c0",
   "metadata": {},
   "source": [
    "## 1-2. AutoModel을 이용하는 방식\n",
    "모델에 관한 정보를 처음부터 명시하지 않아도 되어 조금 유용하게 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0709f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:42.976726Z",
     "start_time": "2021-05-08T04:13:40.017941Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_tf_bert.TFBertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050e149",
   "metadata": {},
   "source": [
    "- `bert-base-cased` : model ID, Huggingface가 지원하는 다양한 pretrained model이 있는데 이들 중 어느 것을 선택할지를 결정하기 위해 이 ID를 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c3f4b",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# 2. Tokenizer\n",
    "- tokenizer 또한 직접 명시하여 내가 사용할 것을 지정해주거나, AutoTokenizer를 사용하여 이미 구비된 model에 알맞는 tokenizer를 자동으로 불러올 수도 있다.\n",
    "- 이때 유의할 점은, **model을 사용할 때 명시했던 것과 동일한 ID로 tokenizer를 생성**해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59450383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:46.897240Z",
     "start_time": "2021-05-08T04:13:42.978212Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a26d3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:51.376494Z",
     "start_time": "2021-05-08T04:13:46.899014Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1abcd4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:51.380337Z",
     "start_time": "2021-05-08T04:13:51.377759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 5960, 1111, 170, 11093, 1883, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 이 경우 BERT의 tokenizer이기 때문에 인코딩이 된 input_ids 뿐만 아니라,\n",
    "# token_type_ids와 attention_mask까지 모두 생성된 input 객체를 받아볼 수 있다.\n",
    "encoded = tokenizer(\"This is Test for aiffel\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d68e138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:51.384581Z",
     "start_time": "2021-05-08T04:13:51.381481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102], [101, 1262, 1330, 5650, 102], [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer는 batch 단위로 input을 받을 수도 있다.\n",
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]\n",
    "\n",
    "encoded_batch = tokenizer(batch_sentences)\n",
    "print(encoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d179d562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:51.389552Z",
     "start_time": "2021-05-08T04:13:51.386327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[ 101, 8667,  146,  112,  182,  170, 1423, 5650,  102],\n",
      "       [ 101, 1262, 1330, 5650,  102,    0,    0,    0,    0],\n",
      "       [ 101, 1262, 1103, 1304, 1304, 1314, 1141,  102,    0]],\n",
      "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 9), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "# tokenize할 때 padding, truncation 등 다양한 옵션 설정 가능\n",
    "# 모델이 어떤 프레임워크(텐서플로우 or PyTorch)에 따라 input 타입을 변경시켜주는 return_tensors 인자도 있다.\n",
    "batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f7e31",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# 3. Processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8743e7f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:51.395323Z",
     "start_time": "2021-05-08T04:13:51.390813Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"sequence classification을 위해 data를 처리하는 기본 processor\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        # tensor dict에서 example을 가져오는 메소드\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        # train data에서 InputExample 클래스를 가지고 있는 것들을 모으는 메소드\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        # dev data(validation data)에서 InputExample 클래스를 가지고 있는 것들을 모으는 메소드\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"test data에서 InputExample 클래스를 가지고 있는 것들을 모으는 메소드\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"data set에 사용되는 라벨들을 리턴하는 메소드\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example):\n",
    "        \"\"\"\n",
    "        tfds(tensorflow-datasets)에서 불러온 데이터를 DataProcessor에 알맞게 가공해주는 메소드\n",
    "        \"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"tab으로 구분된 .tsv파일을 읽어들이는 클래스 메소드\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894438de",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# 4. Config\n",
    "- config는 모델을 학습시키기 위한 요소들을 명시한 json파일로 되어있다.\n",
    "- batch size, learning rate, weight_decay등 train에 필요한 요소들부터 tokenizer에 특수 토큰(special token eg.[MASK])들을 미리 설정하는 등 설정에 관한 전반적인 것들이 명시되어 있다.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149dbf1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:52.194847Z",
     "start_time": "2021-05-08T04:13:51.396649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "460085d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:53.103259Z",
     "start_time": "2021-05-08T04:13:52.199394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91850b1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T04:13:56.149280Z",
     "start_time": "2021-05-08T04:13:53.107943Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "config = model.config\n",
    "print(config.__class__)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d300c",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# 5. Trainer\n",
    "- 모델을 학습시키기 위한 클래스\n",
    "- training, fine-tuning, evaluation 모두 trainer class를 이용하여 할 수 있다.\n",
    "\n",
    "\n",
    "- tensorflow의 경우 tf.keras.model API를 이용하여서도 Huggingface를 통해 불러온 모델을 활용해 학습이나 테스트를 진행할 수 있다. (model.fit(), model.predict()를 활용하는 것이 가능)\n",
    "- `TFTrainer`를 이용할 경우에는 TrainingArguments 를 통해 Huggingface 프레임워크에서 제공하는 기능들을 통합적으로 커스터마이징하여 모델을 학습시킬 수 있다는 장점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae80067c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T07:37:21.863153Z",
     "start_time": "2021-05-08T07:37:06.514659Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForPreTraining.\n",
      "\n",
      "All the layers of TFBertForPreTraining were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForPreTraining for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f54cbe84980>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f54cbe84980>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "=====Results=====\n",
      "TFBertForPreTrainingOutput(loss=None, prediction_logits=array([[[ -7.40272  ,  -7.36266  ,  -7.4500136, ...,  -6.1955214,\n",
      "          -5.8948064,  -6.3672686],\n",
      "        [ -7.8287234,  -8.0582285,  -7.8642063, ...,  -6.419409 ,\n",
      "          -6.3024373,  -6.7624664],\n",
      "        [-11.549929 , -11.551902 , -11.484693 , ...,  -8.114803 ,\n",
      "          -8.314194 ,  -9.444444 ],\n",
      "        ...,\n",
      "        [ -3.2660654,  -3.741642 ,  -2.5797937, ...,  -4.0109973,\n",
      "          -2.4964373,  -3.0753877],\n",
      "        [-12.231966 , -12.027047 , -11.797831 , ...,  -8.838842 ,\n",
      "          -9.091652 , -10.497253 ],\n",
      "        [-10.639944 , -11.074339 , -11.036097 , ...,  -8.1484585,\n",
      "          -9.585199 , -10.671507 ]]], dtype=float32), seq_relationship_logits=array([[ 1.6309199, -0.7168468]], dtype=float32), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Huggingface를 통해 불러온 모델을 tf.keras.model API를 이용해 활용\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForPreTraining, AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased')\n",
    "\n",
    "sentence = \"Hello, This is test for bert TFmodel.\"\n",
    "\n",
    "input_ids = tf.constant(tokenizer.encode(sentence, add_special_tokens=True))[None, :]  # Batch size 1\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "pred = model.predict(input_ids)\n",
    "\n",
    "print(\"=====Results=====\")\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a009e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T09:16:07.074975Z",
     "start_time": "2021-05-08T09:16:06.854868Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c34e73501580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m from transformers import (\n\u001b[1;32m      7\u001b[0m     \u001b[0mTFAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Dict, Optional\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import (\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    glue_convert_examples_to_features,\n",
    ")\n",
    "\n",
    "# TFTrainingArguments 정의\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output이 저장될 경로\n",
    "    num_train_epochs=1,              # train 시킬 총 epochs\n",
    "    per_device_train_batch_size=16,  # 각 device 당 batch size\n",
    "    per_device_eval_batch_size=64,   # evaluation 시에 batch size\n",
    "    warmup_steps=500,                # learning rate scheduler에 따른 warmup_step 설정\n",
    "    weight_decay=0.01,               # weight decay\n",
    "    logging_dir='./logs',            # log가 저장될 경로\n",
    "    do_train=True,                   # train 수행여부\n",
    "    do_eval=True,                    # eval 수행여부\n",
    ")\n",
    "\n",
    "# model, tokenizer 생성\n",
    "model_name_or_path = 'bert-base-uncased'\n",
    "with training_args.strategy.scope():    # training_args가 영향을 미치는 model의 범위를 지정\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            from_pt=bool(\".bin\" in model_name_or_path),\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdaf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "ds, info = tfds.load('glue/mrpc', with_info=True)\n",
    "train_dataset = glue_convert_examples_to_features(ds['train'], tokenizer, 128, 'mrpc')\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.assert_cardinality(info.splits['train'].num_examples))\n",
    "\n",
    "# TFTrainer 생성\n",
    "trainer = TFTrainer(\n",
    "    model=model,                          # 학습시킬 model\n",
    "    args=training_args,                  # TFTrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train_dataset,   # training dataset\n",
    ")\n",
    "\n",
    "# 학습 진행\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
