{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영어-스페인어 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm    # tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_file() : URL로부터 데이터를 다운받고, 압축된 형식일 경우 해제까지 알아서 해줌\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 118964\n",
      "Example:\n",
      ">> Go.\tVe.\n",
      ">> Wait.\tEsperen.\n",
      ">> Hug me.\tAbrázame.\n",
      ">> No way!\t¡Ni cagando!\n",
      ">> Call me.\tLlamame.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터는 `\\t` 기호를 기준으로 영어와 스페인어가 병렬 쌍을 이루고 있다.\n",
    "- `\\t` 를 기준으로 split() 함수를 써써 소스 문장과 타겟 문장을 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: go away !\n",
      "Spanish: <start> salga de aqu ! <end>\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "for pair in raw[:num_examples]:\n",
    "    eng, spa = pair.split(\"\\t\")\n",
    "\n",
    "    enc_corpus.append(preprocess_sentence(eng))\n",
    "    dec_corpus.append(preprocess_sentence(spa, s_token=True, e_token=True))\n",
    "\n",
    "print(\"English:\", enc_corpus[100])   # go away !\n",
    "print(\"Spanish:\", dec_corpus[100])   # <start> salga de aqu ! <end>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size: 4931\n",
      "Spanish Vocab Size: 8893\n"
     ]
    }
   ],
   "source": [
    "# 토큰화하기\n",
    "enc_tensor, enc_tokenizer = tokenize(enc_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(dec_corpus)\n",
    "\n",
    "# 훈련 데이터와 검증 데이터로 분리하기\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
    "\n",
    "print(\"English Vocab Size:\", len(enc_tokenizer.index_word))\n",
    "print(\"Spanish Vocab Size:\", len(dec_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 설계\n",
    "- 각각 1개의 GRU를 갖는 Encoder-Decoder 구조\n",
    "- Encoder : 모든 Time-step의 Hidden state를 출력\n",
    "- Decoder : Encoder의 출력과 Decoder의 t-1 step의 hidden state로 Attention을 취하여, t step의 hidden state를 생성\n",
    "- Decoder에서 t step의 단어로 예측된 것을 실제 정답과 대조해 loss를 구하고, 생성된 t step의 hidden state는 t+1 step의 hidden state를 만들기 위해 다시 decoder에 전달된다.\n",
    "- `t=1`일 때 hidden state 정의 : encoder의 final state를 hidden state로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 8894)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Optimizer, Loss\n",
    "- SparseCategoricalCrossentropy() : 모델이 출력한 확률 분포와 정수 인덱스 답안을 비교해 cross entropy를 구함 ex) \\[0.1, 0.2, 0.7\\] & 2\n",
    "  - from_logits : 확률분포가 softmax를 거쳐서 들어오는지, 모델의 출력값 그대로 들어오는지(True)를 결정\n",
    "\n",
    "\n",
    "- tf.math.logical_not()\n",
    "  - args : bool유형의 tensor\n",
    "  - return : bool유형의 tensor\n",
    "```py\n",
    "tf.math.logical_not(tf.constant([True, False]))\n",
    "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n",
    "```\n",
    "\n",
    "- tf.cast()\n",
    "  - tensor를 새로운 형태로 캐스팅하는데 사용한다.\n",
    "  - 부동소수점형에서 정수형으로 바꾼 경우 소수점을 버린다.\n",
    "  - bool형인 경우 true이면 1, false이면 0을 출력한다.\n",
    "  \n",
    "  \n",
    "- tf.reduce_mean()\n",
    "  - 두번째 인자를 적지 않은 경우 주어진 배열 전체 원소의 합을 원소 개수로 나눔(즉, 평균)\n",
    "  - 두번째 인자 - 0 : 열 단위로 평균\n",
    "  - 두번째 인자 - 1 : 행 단위로 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  # 패딩을 위한 토큰이라고 명시하기 위하여\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=bool, numpy=array([ True,  True, False, False, False])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,0,1,2,3]\n",
    "tf.math.equal(a, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. train_step 구현\n",
    "train_step() : 학습에 필요한 것을 모두 가져가 Loss를 계산한 후 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # 텐서플로우 연산을 GPU에서 동작하게 해 훈련을 가속할 수 있도록 도와줌\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    # 학습하며 발생한 모든 연산을 기록하는 테이프로\n",
    "    # 모델이 각 스텝의 최종 단계에서 미분값을 구하는 데에 사용됨\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)  # encoder에 소스문장을 전달하여 컨텍스트 벡터인 enc_out을 생성\n",
    "        h_dec = enc_out[:, -1]  # encoder의 마지막을 가져와(encoder의 final state) decoder의 hidden state로 정의 (t=0일때)\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1) # Decoder에 입력으로 전달할 <start> 토큰 문장 생성\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            # pred : <start> 문장과 enc_out, Hidden State를 기반으로 다음 단어(t=1)를 예측\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            # 예측된 단어와 정답 간의 Loss를 구한 후, t=1의 정답 단어를 다음 입력으로 사용 (예측 단어 X) -> 그 후 반복!\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. 훈련 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 375/375 [08:14<00:00,  1.32s/it, Loss 1.3409]\n",
      "Epoch  2: 100%|██████████| 375/375 [08:07<00:00,  1.30s/it, Loss 0.8788]\n",
      "Epoch  3: 100%|██████████| 375/375 [07:31<00:00,  1.20s/it, Loss 0.6090]\n",
      "Epoch  4: 100%|██████████| 375/375 [07:42<00:00,  1.23s/it, Loss 0.4339]\n",
      "Epoch  5: 100%|██████████| 375/375 [07:12<00:00,  1.15s/it, Loss 0.3232]\n",
      "Epoch  6: 100%|██████████| 375/375 [07:28<00:00,  1.20s/it, Loss 0.2514]\n",
      "Epoch  7: 100%|██████████| 375/375 [07:57<00:00,  1.27s/it, Loss 0.2054]\n",
      "Epoch  8: 100%|██████████| 375/375 [08:10<00:00,  1.31s/it, Loss 0.1756]\n",
      "Epoch  9: 100%|██████████| 375/375 [08:10<00:00,  1.31s/it, Loss 0.1566]\n",
      "Epoch 10: 100%|██████████| 375/375 [08:10<00:00,  1.31s/it, Loss 0.1396]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))  #  각 배치의 시작 인덱스를 idx_list 배열에 저장\n",
    "    random.shuffle(idx_list)  # 인덱스를 섞어서 처리\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    # 각 미니배치를 train_step()에서 학습 -> loss를 계산한 후 반환하는 함수\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "\n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 375/375 [08:10<00:00,  1.31s/it, Loss 0.1295]\n",
      "Test Epoch  1: 100%|██████████| 94/94 [00:34<00:00,  2.70it/s, Test Loss 0.7028]\n",
      "Epoch  2: 100%|██████████| 375/375 [08:10<00:00,  1.31s/it, Loss 0.1195]\n",
      "Test Epoch  2: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s, Test Loss 0.7088]\n",
      "Epoch  3: 100%|██████████| 375/375 [07:42<00:00,  1.23s/it, Loss 0.1127]\n",
      "Test Epoch  3: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s, Test Loss 0.7214]\n",
      "Epoch  4: 100%|██████████| 375/375 [08:11<00:00,  1.31s/it, Loss 0.1091]\n",
      "Test Epoch  4: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s, Test Loss 0.7327]\n",
      "Epoch  5: 100%|██████████| 375/375 [07:40<00:00,  1.23s/it, Loss 0.1027]\n",
      "Test Epoch  5: 100%|██████████| 94/94 [00:23<00:00,  3.97it/s, Test Loss 0.7471]\n",
      "Epoch  6: 100%|██████████| 375/375 [08:06<00:00,  1.30s/it, Loss 0.1016]\n",
      "Test Epoch  6: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s, Test Loss 0.7564]\n",
      "Epoch  7: 100%|██████████| 375/375 [07:20<00:00,  1.17s/it, Loss 0.0965]\n",
      "Test Epoch  7: 100%|██████████| 94/94 [00:22<00:00,  4.27it/s, Test Loss 0.7566]\n",
      "Epoch  8: 100%|██████████| 375/375 [07:43<00:00,  1.24s/it, Loss 0.0921]\n",
      "Test Epoch  8: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s, Test Loss 0.7669]\n",
      "Epoch  9: 100%|██████████| 375/375 [08:10<00:00,  1.31s/it, Loss 0.0913]\n",
      "Test Epoch  9: 100%|██████████| 94/94 [00:25<00:00,  3.74it/s, Test Loss 0.7635]\n",
      "Epoch 10: 100%|██████████| 375/375 [08:11<00:00,  1.31s/it, Loss 0.0879]\n",
      "Test Epoch 10: 100%|██████████| 94/94 [00:25<00:00,  3.74it/s, Test Loss 0.7729]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "\n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. 번역 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac14/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/home/ssac14/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: can i have some coffee ?\n",
      "Predicted translation: tendr tiempo de caf ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 608,
       "width": 610
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
